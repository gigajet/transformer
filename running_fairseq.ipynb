{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gigajet/transformer/blob/master/running_fairseq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OBkDDiNQPuD"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C-uhHC2jy04F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d21cfc-94b8-409c-a2b2-e79116d02d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Google Colab doesn't support `ln`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaNm6BzuuVW8",
        "outputId": "1f3d2bfb-599f-4ccd-d010-a8d04881bfaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 31273, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 31273 (delta 18), reused 45 (delta 18), pack-reused 31225\u001b[K\n",
            "Receiving objects: 100% (31273/31273), 21.53 MiB | 15.61 MiB/s, done.\n",
            "Resolving deltas: 100% (23055/23055), done.\n",
            "/content/fairseq\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+c8d6fb1) (1.15.0)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+c8d6fb1) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+c8d6fb1) (1.21.6)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting bitarray\n",
            "  Downloading bitarray-2.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[K     |████████████████████████████████| 236 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+c8d6fb1) (4.64.0)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+c8d6fb1) (0.11.0+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+c8d6fb1) (2019.12.20)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+c8d6fb1) (0.29.28)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+c8d6fb1) (5.7.1)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 57.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+c8d6fb1) (4.2.0)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+c8d6fb1) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+c8d6fb1) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+c8d6fb1) (3.8.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=e76f532b39ec7272f3a65ceee14e364e009f55d442272272f8c85162c8a70b6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.5.0 colorama-0.4.4 fairseq hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.4.0 sacrebleu-2.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install fairseq\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd fairseq\n",
        "!pip install --editable ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMStVQCYu-nN",
        "outputId": "908122a2-4c85-403e-f5fd-4592a7d4b996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastBPE\n",
            "  Downloading fastBPE-0.1.0.tar.gz (35 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 18.0 MB/s \n",
            "\u001b[?25hCollecting subword_nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Building wheels for collected packages: fastBPE, sacremoses\n",
            "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp37-cp37m-linux_x86_64.whl size=483164 sha256=3d21ad62e7dbba248531515a9287ab9211022fd196aa700aee5e680eb56b808c\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/d4/0e/0d317a65f77d3f8049fedd8a2ee0519164cf3e6bd77ef886f1\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=e685ad23ec88de4592b31633a5727df9b9374c0555dd25ba24de1e2fe1d18119\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built fastBPE sacremoses\n",
            "Installing collected packages: mock, subword-nmt, sacremoses, fastBPE\n",
            "Successfully installed fastBPE-0.1.0 mock-4.0.3 sacremoses-0.0.53 subword-nmt-0.3.8\n"
          ]
        }
      ],
      "source": [
        "# Dependencies for preprocessing\n",
        "!pip install fastBPE sacremoses subword_nmt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CD6pa7A0CgRX"
      },
      "outputs": [],
      "source": [
        "# Run this if you get error \"importlib_metadata.PackageNotFoundError: No package metadata was found for fairseq\" in next cell\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBYOOYRFbojI"
      },
      "source": [
        "## Verify that fairseq is now usable (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "cAecsqc7vGTE",
        "outputId": "3cdf48be-1ce8-482f-b696-0e0e9fcc1302"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-02 04:29:06 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "Downloading: \"https://github.com/pytorch/fairseq/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n",
            "2022-05-02 04:29:10 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2 not found in cache, downloading to /tmp/tmpy9uuosu6\n",
            "  7%|▋         | 150277120/2193287384 [00:05<01:50, 18416384.24B/s]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-08ea59ea7e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Note: WMT'19 models use fastBPE instead of subword_nmt, see instructions below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt16.en-de',\n\u001b[0;32m---> 12\u001b[0;31m                        tokenizer='moses', bpe='subword_nmt')\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0men2de\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# disable dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mrepo_or_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_cache_or_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fairseq/fairseq/models/fairseq_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_name_or_path, checkpoint_file, data_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mdata_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0marchive_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         )\n\u001b[1;32m    274\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name_or_path, checkpoint_file, data_name_or_path, archive_map, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_archive_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# convenience hack for loading data and BPE codes from model archive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fairseq/fairseq/file_utils.py\u001b[0m in \u001b[0;36mload_archive_file\u001b[0;34m(archive_file)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# redirect to the cache, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         logger.info(\n",
            "\u001b[0;32m/content/fairseq/fairseq/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# File, and it exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fairseq/fairseq/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0ms3_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# we are copying the file before closing it, so flush to avoid truncation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fairseq/fairseq/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontent_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;31m# Amount is not given (unbounded read) so we must check self.length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# This cell is for verifying the install\n",
        "# Interactive translation\n",
        "import torch\n",
        "import fairseq\n",
        "\n",
        "# List available models\n",
        "torch.hub.list('pytorch/fairseq')  # [..., 'transformer.wmt16.en-de', ... ]\n",
        "\n",
        "# Load a transformer trained on WMT'16 En-De\n",
        "# Note: WMT'19 models use fastBPE instead of subword_nmt, see instructions below\n",
        "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt16.en-de',\n",
        "                       tokenizer='moses', bpe='subword_nmt')\n",
        "en2de.eval()  # disable dropout\n",
        "\n",
        "# The underlying model is available under the *models* attribute\n",
        "assert isinstance(en2de.models[0], fairseq.models.transformer.TransformerModel)\n",
        "\n",
        "# Move model to GPU for faster translation\n",
        "# en2de.cuda()\n",
        "\n",
        "# Translate a sentence\n",
        "en2de.translate('Hello world!')\n",
        "# 'Hallo Welt!'\n",
        "\n",
        "# Batched translation\n",
        "en2de.translate(['Hello world!', 'The cat sat on the mat.'])\n",
        "# ['Hallo Welt!', 'Die Katze saß auf der Matte.']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuVoveca1ATX"
      },
      "source": [
        "## Prepare & Train IWSLT'14 German to English (Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9uIZOA0-03u",
        "outputId": "2ccdda54-e869-4bca-a1b9-643a7abbc499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/fairseq/examples/translation\n",
            "Cloning Moses github repository (for tokenization scripts)...\n",
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 148090, done.\u001b[K\n",
            "remote: Counting objects: 100% (518/518), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "^C\n",
            "/content/fairseq\n"
          ]
        }
      ],
      "source": [
        "# Download and prepare the data\n",
        "%cd examples/translation/\n",
        "!bash prepare-iwslt14.sh\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01aLMot8vkby",
        "outputId": "9b37e8e5-642f-4acf-ade8-a003a8f293b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: TEXT=examples/translation/iwslt14.tokenized.de-en\n",
            "/bin/bash: fairseq-preprocess: command not found\n"
          ]
        }
      ],
      "source": [
        "# Preprocess/binarize the data\n",
        "# Look below on how to set environment variable in GG Colab\n",
        "%env TEXT=examples/translation/iwslt14.tokenized.de-en\n",
        "!fairseq-preprocess --source-lang de --target-lang en \\\n",
        "    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
        "    --destdir /content/drive/MyDrive/translation/iwslt14.tokenized.de-en \\\n",
        "    --workers 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6m9cR6h1HPM",
        "outputId": "398179fe-af5e-41a9-bd05-7c4a8b8808ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "/bin/bash: fairseq-train: command not found\n"
          ]
        }
      ],
      "source": [
        "# Train the model, please change --max-epoch depends whether you use gpu/cpu\n",
        "%env CUDA_VISIBLE_DEVICES=0 \n",
        "!fairseq-train \\\n",
        "    /content/drive/MyDrive/translation/iwslt14.tokenized.de-en \\\n",
        "    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-epoch 15 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIVXq97t1JnF",
        "outputId": "ba78b363-c3a9-461d-de5d-a3f97d791a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: fairseq-generate: command not found\n"
          ]
        }
      ],
      "source": [
        "# Evaluate our model\n",
        "!fairseq-generate /content/drive/MyDrive/translation/iwslt14.tokenized.de-en \\\n",
        "    --path /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/checkpoints/checkpoint_best.pt \\\n",
        "    --batch-size 128 --beam 5 --remove-bpe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYLNU2e0Ow0d"
      },
      "source": [
        "## Prepare IWSLT'15 English - Vietnamese data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RphxFkxj6wIW",
        "outputId": "68da7334-7d27-414d-839f-702ab9276398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/translation/en-vi.zip\n",
            "  inflating: en-vi/IWSLT15.TED.dev2010.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.dev2010.en-vi.vi  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2010.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2010.en-vi.vi  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2011.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2011.en-vi.vi  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2012.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2012.en-vi.vi  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2013.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2013.en-vi.vi  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2015.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2015.en-vi.vi  \n",
            "  inflating: en-vi/train.en          \n",
            "  inflating: en-vi/train.en-vi       \n",
            "  inflating: en-vi/train.tags.en-vi.clean.en  \n",
            "  inflating: en-vi/train.tags.en-vi.clean.vi  \n",
            "  inflating: en-vi/train.tags.en-vi.en  \n",
            "  inflating: en-vi/train.tags.en-vi.tok.en  \n",
            "  inflating: en-vi/train.tags.en-vi.tok.vi  \n",
            "  inflating: en-vi/train.tags.en-vi.vi  \n",
            "  inflating: en-vi/train.vi          \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/translation/en-vi.zip -d en-vi\n",
        "!cp /content/drive/MyDrive/translation/myprepare-iwslt15-en-vi.sh ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdtKEvLYcpP1",
        "outputId": "a34c47bb-1f8a-4354-ae95-acd51a06734b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning Moses github repository (for tokenization scripts)...\n",
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 148090, done.\u001b[K\n",
            "remote: Counting objects: 100% (518/518), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "remote: Total 148090 (delta 319), reused 443 (delta 292), pack-reused 147572\u001b[K\n",
            "Receiving objects: 100% (148090/148090), 129.87 MiB | 18.58 MiB/s, done.\n",
            "Resolving deltas: 100% (114345/114345), done.\n",
            "Cloning Subword NMT repository (for BPE pre-processing)...\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 590, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 590 (delta 3), reused 4 (delta 1), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (590/590), 245.03 KiB | 4.90 MiB/s, done.\n",
            "Resolving deltas: 100% (352/352), done.\n",
            "Creating backup of train.en train.vi\n",
            "creating train, valid, test...\n",
            "learn_bpe.py on en-vi/train.en-vi...\n",
            "100% 10000/10000 [00:12<00:00, 813.07it/s]\n",
            "apply_bpe.py to train.en...\n",
            "apply_bpe.py to valid.en...\n",
            "apply_bpe.py to test.en...\n",
            "apply_bpe.py to train.vi...\n",
            "apply_bpe.py to valid.vi...\n",
            "apply_bpe.py to test.vi...\n"
          ]
        }
      ],
      "source": [
        "!sh myprepare-iwslt15-en-vi.sh\n",
        "# After this, we have `train valid test` in iwslt15.tokenized.en-vi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLUymR6GeKvp",
        "outputId": "30c38f1e-9e09-40c8-a7fe-9067bf2483a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TEXT=iwslt15.tokenized.en-vi\n",
            "2022-05-06 15:02:56 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-05-06 15:02:56 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/content/drive/MyDrive/iwslt15.tokenized.en-vi/dataset', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='vi', task='translation', tensorboard_logdir=None, testpref='iwslt15.tokenized.en-vi/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='iwslt15.tokenized.en-vi/train', use_plasma_view=False, user_dir=None, validpref='iwslt15.tokenized.en-vi/valid', wandb_project=None, workers=20)\n",
            "2022-05-06 15:03:11 | INFO | fairseq_cli.preprocess | [en] Dictionary: 7656 types\n",
            "2022-05-06 15:03:23 | INFO | fairseq_cli.preprocess | [en] iwslt15.tokenized.en-vi/train.en: 111966 sents, 2768010 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-06 15:03:23 | INFO | fairseq_cli.preprocess | [en] Dictionary: 7656 types\n",
            "2022-05-06 15:03:24 | INFO | fairseq_cli.preprocess | [en] iwslt15.tokenized.en-vi/valid.en: 5089 sents, 128011 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-06 15:03:24 | INFO | fairseq_cli.preprocess | [en] Dictionary: 7656 types\n",
            "2022-05-06 15:03:26 | INFO | fairseq_cli.preprocess | [en] iwslt15.tokenized.en-vi/test.en: 7446 sents, 176932 tokens, 0.00339% replaced (by <unk>)\n",
            "2022-05-06 15:03:26 | INFO | fairseq_cli.preprocess | [vi] Dictionary: 6656 types\n",
            "2022-05-06 15:03:44 | INFO | fairseq_cli.preprocess | [vi] iwslt15.tokenized.en-vi/train.vi: 111966 sents, 2971232 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-06 15:03:44 | INFO | fairseq_cli.preprocess | [vi] Dictionary: 6656 types\n",
            "2022-05-06 15:03:46 | INFO | fairseq_cli.preprocess | [vi] iwslt15.tokenized.en-vi/valid.vi: 5089 sents, 137095 tokens, 0.0277% replaced (by <unk>)\n",
            "2022-05-06 15:03:46 | INFO | fairseq_cli.preprocess | [vi] Dictionary: 6656 types\n",
            "2022-05-06 15:03:47 | INFO | fairseq_cli.preprocess | [vi] iwslt15.tokenized.en-vi/test.vi: 7446 sents, 197843 tokens, 0.16% replaced (by <unk>)\n",
            "2022-05-06 15:03:47 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/drive/MyDrive/iwslt15.tokenized.en-vi/dataset\n"
          ]
        }
      ],
      "source": [
        "# Preprocess/binarize the data\n",
        "# Look below on how to set environment variable in GG Colab\n",
        "%env TEXT=iwslt15.tokenized.en-vi\n",
        "!fairseq-preprocess --source-lang en --target-lang vi \\\n",
        "    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
        "    --destdir /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset \\\n",
        "    --workers 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsmOKipnchcY"
      },
      "source": [
        "## Train IWSLT'15 English-Vietnamese (Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXd1Mv0Q7Cez",
        "outputId": "353dcaef-9d27-439a-c4d9-771c89682cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "/bin/bash: fairseq-train: command not found\n"
          ]
        }
      ],
      "source": [
        "# arch transformer, ở đây dùng transformer_iwslt_de_en là dùng lại architecture của nó\n",
        "# Train the model, please change --max-epoch depends whether you use gpu/cpu\n",
        "# see stop-time-hours\n",
        "%env CUDA_VISIBLE_DEVICES=0 \n",
        "!fairseq-train \\\n",
        "    /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset \\\n",
        "    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-epoch 36 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir checkpoints\n",
        "    # --save-dir /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE1kX47f3YkB",
        "outputId": "1edfd8f9-9846-4711-edb4-043a0f6aa4a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: fairseq-generate: command not found\n"
          ]
        }
      ],
      "source": [
        "# Evaluate our model\n",
        "!fairseq-generate /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset \\\n",
        "    --path /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/checkpoints/checkpoint_best.pt \\\n",
        "    --batch-size 128 --beam 5 --remove-bpe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Wbr-bJW4-r"
      },
      "source": [
        "## Try to add FuzzyLayer to Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iagQ0QWRb4n7",
        "outputId": "70eb4f54-b48b-4bce-fdc5-a7a77ae583fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 35])\n",
            "tensor([[6.7112e-06, 6.8293e-04, 1.1360e-05, 1.4338e-05, 3.1253e-06, 4.4289e-04,\n",
            "         1.2499e-04, 2.6953e-04, 5.7319e-06, 3.1216e-05, 5.5655e-05, 3.2802e-04,\n",
            "         3.7528e-02, 1.1846e-05, 7.1205e-07, 2.0275e-06, 7.1357e-07, 4.7466e-04,\n",
            "         1.9231e-06, 2.3387e-03, 8.4445e-06, 1.2212e-04, 1.2321e-04, 4.9719e-04,\n",
            "         9.7817e-03, 7.2931e-03, 2.3511e-06, 5.0083e-02, 1.0146e-05, 1.0202e-06,\n",
            "         5.4978e-04, 8.7630e-04, 9.0879e-02, 2.8873e-08, 5.0999e-03],\n",
            "        [6.7112e-06, 6.8293e-04, 1.1360e-05, 1.4338e-05, 3.1253e-06, 4.4289e-04,\n",
            "         1.2499e-04, 2.6953e-04, 5.7319e-06, 3.1216e-05, 5.5655e-05, 3.2802e-04,\n",
            "         3.7528e-02, 1.1846e-05, 7.1205e-07, 2.0275e-06, 7.1357e-07, 4.7466e-04,\n",
            "         1.9231e-06, 2.3387e-03, 8.4445e-06, 1.2212e-04, 1.2321e-04, 4.9719e-04,\n",
            "         9.7817e-03, 7.2931e-03, 2.3511e-06, 5.0083e-02, 1.0146e-05, 1.0202e-06,\n",
            "         5.4978e-04, 8.7630e-04, 9.0879e-02, 2.8873e-08, 5.0999e-03],\n",
            "        [6.7112e-06, 6.8293e-04, 1.1360e-05, 1.4338e-05, 3.1253e-06, 4.4289e-04,\n",
            "         1.2499e-04, 2.6953e-04, 5.7319e-06, 3.1216e-05, 5.5655e-05, 3.2802e-04,\n",
            "         3.7528e-02, 1.1846e-05, 7.1205e-07, 2.0275e-06, 7.1357e-07, 4.7466e-04,\n",
            "         1.9231e-06, 2.3387e-03, 8.4445e-06, 1.2212e-04, 1.2321e-04, 4.9719e-04,\n",
            "         9.7817e-03, 7.2931e-03, 2.3511e-06, 5.0083e-02, 1.0146e-05, 1.0202e-06,\n",
            "         5.4978e-04, 8.7630e-04, 9.0879e-02, 2.8873e-08, 5.0999e-03]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# FuzzyLayer\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "\n",
        "class FuzzyLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim: int, **kwargs):\n",
        "        super(FuzzyLayer, self).__init__(**kwargs)\n",
        "        self.output_dim = output_dim\n",
        "        self.fuzzy_degree = nn.Parameter(torch.empty(output_dim))\n",
        "        self.sigma = nn.Parameter(torch.ones(output_dim))\n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self) -> None:\n",
        "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
        "        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
        "        # https://github.com/pytorch/pytorch/issues/57109\n",
        "        nn.init.uniform_(self.fuzzy_degree)\n",
        "\n",
        "    def forward(self, input, **kwargs):\n",
        "        x = torch.repeat_interleave(torch.unsqueeze(input,-1), self.output_dim, dim=-1)\n",
        "        fuzzy_out = torch.exp(\n",
        "                      -torch.sum(\n",
        "                          torch.square((x-self.fuzzy_degree)/(self.sigma**2))            \n",
        "                          ,dim=-2, keepdims=False)\n",
        "              )\n",
        "        return fuzzy_out\n",
        "\n",
        "\n",
        "class FuzzyRuleLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim: int, output_dim,**kwargs):\n",
        "        super(FuzzyRuleLayer, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.layers = nn.ModuleList([\n",
        "            FuzzyLayer(output_dim) for _ in range(input_dim)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, input_dim = input.size()\n",
        "        an=torch.ones(batch_size, self.output_dim)\n",
        "        for layer in self.layers:\n",
        "            an=an*layer(input)\n",
        "        return an\n",
        "\n",
        "z=FuzzyRuleLayer(5,35)\n",
        "y=torch.ones(3,5)\n",
        "print(z(y).shape)\n",
        "print(z(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "227Lz-QqbOQK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icDOr3H9HlHY"
      },
      "source": [
        "## Try to add MyLSTM to fairseq (CPU only)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnFBO2ZoZoWu"
      },
      "source": [
        "### Some notes\n",
        "+ Follow the tutorial at https://fairseq.readthedocs.io/en/latest/tutorial_simple_lstm.html, add `@register_model` and `@register_model_architecture` to appropriate class.\n",
        "\n",
        "+ That's not all, you must either:\n",
        "\n",
        "1. Move your `model_name.py` to `fairseq/models`\n",
        "\n",
        "2. Inside your directory (supposed `user_dir`), create a new folder named `models` and move your file there. In the command line (such as `fairseq-train`), specify `--user-dir /path/to/user_dir`\n",
        "\n",
        "Because each time the command line tool runs, it must run some bootstrap code to get the current model list, parse your arguments, etc. Therefore you must follow some of their rules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85WWH0DhawMB",
        "outputId": "4da5f279-965c-41fd-bbc1-a1b8ac8e02a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'my_transformer' already exists and is not an empty directory.\n",
            "mkdir: cannot create directory ‘mymodel’: File exists\n",
            "/content/mymodel\n",
            "mkdir: cannot create directory ‘models’: File exists\n",
            "/content/mymodel/models\n"
          ]
        }
      ],
      "source": [
        "# Make mymodel path and prepare my_model (in this case `MyLSTM.py`) in user_dir (in this case `mymodel`)\n",
        "%cd /content\n",
        "!rm -rf my_transformer\n",
        "!git clone https://github.com/gigajet/transformer my_transformer\n",
        "!mkdir mymodel\n",
        "%cd mymodel\n",
        "!mkdir models\n",
        "%cd models \n",
        "!cp /content/my_transformer/MyFairseqLSTM.py .\n",
        "!cp -r /content/my_transformer/layer .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbnkII_hHqIj",
        "outputId": "b383bf6e-a5b0-413a-9b45-510b0367167c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "2022-04-22 10:02:58 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2022-04-22 10:02:59 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/content/mymodel', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 12000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='mylstm_default', adam_betas=(0.9, 0.999), adam_eps=1e-08, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='mylstm_default', azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/content/drive/MyDrive/translation/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_dropout=0.2, decoder_embed_dim=256, decoder_hidden_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_dropout=0.2, encoder_embed_dim=256, encoder_hidden_dim=256, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.005], lr_scheduler='fixed', lr_shrink=0.5, max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=12000, max_tokens_valid=12000, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/content/mymodel', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=0, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/content/drive/MyDrive/translation/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.005]}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.5, 'warmup_updates': 0, 'lr': [0.005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-04-22 10:03:01 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types\n",
            "2022-04-22 10:03:01 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types\n",
            "MyLSTM(\n",
            "  (encoder): SimpleLSTMEncoder(\n",
            "    (embed_tokens): Embedding(8848, 256, padding_idx=1)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "  )\n",
            "  (decoder): SimpleLSTMDecoder(\n",
            "    (embed_tokens): Embedding(6632, 256, padding_idx=1)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (lstm): LSTM(512, 256)\n",
            "    (output_projection): Linear(in_features=256, out_features=6632, bias=True)\n",
            "  )\n",
            ")\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | MyLSTM(\n",
            "  (encoder): SimpleLSTMEncoder(\n",
            "    (embed_tokens): Embedding(8848, 256, padding_idx=1)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "  )\n",
            "  (decoder): SimpleLSTMDecoder(\n",
            "    (embed_tokens): Embedding(6632, 256, padding_idx=1)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (lstm): LSTM(512, 256)\n",
            "    (output_projection): Linear(in_features=256, out_features=6632, bias=True)\n",
            "  )\n",
            ")\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | model: MyLSTM\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | num. shared model params: 6,982,120 (num. trained: 6,982,120)\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-04-22 10:03:01 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/valid.de-en.de\n",
            "2022-04-22 10:03:01 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/valid.de-en.en\n",
            "2022-04-22 10:03:01 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/translation/iwslt14.tokenized.de-en valid de-en 7283 examples\n",
            "2022-04-22 10:03:04 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-04-22 10:03:04 | INFO | fairseq.utils | rank   0: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               \n",
            "2022-04-22 10:03:04 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-04-22 10:03:04 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-04-22 10:03:04 | INFO | fairseq_cli.train | max tokens per device = 12000 and max sentences per device = None\n",
            "2022-04-22 10:03:04 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
            "2022-04-22 10:03:04 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
            "2022-04-22 10:03:04 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-04-22 10:03:04 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/train.de-en.de\n",
            "2022-04-22 10:03:04 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/train.de-en.en\n",
            "2022-04-22 10:03:04 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/translation/iwslt14.tokenized.de-en train de-en 160239 examples\n",
            "2022-04-22 10:03:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 388\n",
            "epoch 001:   0% 0/388 [00:00<?, ?it/s]2022-04-22 10:03:04 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-04-22 10:03:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/bin/fairseq-train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 316, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 856, in train_step\n",
            "    raise e\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 830, in train_step\n",
            "    **extra_kwargs,\n",
            "  File \"/content/fairseq/fairseq/tasks/fairseq_task.py\", line 512, in train_step\n",
            "    loss, sample_size, logging_output = criterion(model, sample)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/fairseq/fairseq/criterions/cross_entropy.py\", line 35, in forward\n",
            "    net_output = model(**sample[\"net_input\"])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/fairseq/fairseq/models/fairseq_model.py\", line 322, in forward\n",
            "    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/mymodel/models/MyFairseqLSTM.py\", line 60, in forward\n",
            "    x = nn.utils.rnn.pack_padded_sequence(x, src_lengths, batch_first=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/utils/rnn.py\", line 249, in pack_padded_sequence\n",
            "    _VF._pack_padded_sequence(input, lengths, batch_first)\n",
            "RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor\n"
          ]
        }
      ],
      "source": [
        "# Train the model, please change --max-epoch depends whether you use gpu/cpu\n",
        "%env CUDA_VISIBLE_DEVICES=0 \n",
        "!fairseq-train \\\n",
        "    /content/drive/MyDrive/translation/iwslt14.tokenized.de-en \\\n",
        "    --encoder-dropout 0.2 --decoder-dropout 0.2 \\\n",
        "    --optimizer adam --lr 0.005 --lr-shrink 0.5 \\\n",
        "    --max-epoch 1 \\\n",
        "    --max-tokens 12000 \\\n",
        "    --save-dir checkpoints \\\n",
        "    --user-dir /content/mymodel \\\n",
        "    --arch mylstm_default \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wp9VGKqQVPb"
      },
      "source": [
        "## Get mymodel from github and prepare directory structure that satisfies fairseq requirement. Adding custom model to fairseq in `/content/mymodel`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfQorQ23Qf24",
        "outputId": "1d0da3de-27d9-4484-f4ca-6ce43499a77b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'my_transformer'...\n",
            "remote: Enumerating objects: 249, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 249 (delta 1), reused 3 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (249/249), 28.27 MiB | 13.89 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n",
            "/content/mymodel\n",
            "/content/mymodel/models\n",
            "/content/fairseq\n"
          ]
        }
      ],
      "source": [
        "# Make mymodel path and prepare my_model (in this case `MyLSTM.py`) in user_dir (in this case `mymodel`)\n",
        "%cd /content\n",
        "!rm -rf my_transformer\n",
        "!git clone https://github.com/gigajet/transformer my_transformer\n",
        "!rm -rf mymodel\n",
        "!mkdir mymodel\n",
        "%cd mymodel\n",
        "!cp -r /content/my_transformer models\n",
        "\n",
        "# Xóa thể loại không liên quan\n",
        "%cd models\n",
        "!rm -f string_reverser_*.py\n",
        "\n",
        "# Finalize\n",
        "%cd /content/fairseq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hv4UomOGyJqX"
      },
      "outputs": [],
      "source": [
        "# fix the import layer to mymodel.layer\n",
        "# F*** PYTHON3 IMPORT\n",
        "!find /content/mymodel -type f -exec sed -i \"s/from layer\\./from mymodel.models.layer./g\" {} \\;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training nntransformer_default"
      ],
      "metadata": {
        "id": "7DGeQy3fetQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w81ozS3NQf3F",
        "outputId": "535eef74-9720-4d2d-b2e6-f3d54b98b4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "2022-05-06 15:16:16 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-05-06 15:16:18 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/content/mymodel', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='nntransformer_default', adam_betas='(0.9, 0.98)', adam_eps=1e-08, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='nntransformer_default', azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', device_id=0, dim_feedforward=2048, dim_model=512, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=100, max_source_positions=1024, max_src_len=16378, max_target_positions=1024, max_tgt_len=16378, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_head=8, num_layer=6, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/content/mymodel', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-05-06 15:16:18 | INFO | fairseq.tasks.translation | [en] dictionary: 7656 types\n",
            "2022-05-06 15:16:18 | INFO | fairseq.tasks.translation | [vi] dictionary: 6656 types\n",
            "2022-05-06 15:16:22 | INFO | fairseq_cli.train | NNTransformer(\n",
            "  (encoder): NNTransformerEncoder(\n",
            "    (embedding): PositionalEncodedEmbedding(\n",
            "      (input_embedding): Embedding(7656, 512, padding_idx=1)\n",
            "    )\n",
            "    (nn_encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (2): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (3): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (4): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (5): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): NNTransformerDecoder(\n",
            "    (embedding): PositionalEncodedEmbedding(\n",
            "      (input_embedding): Embedding(6656, 512, padding_idx=1)\n",
            "    )\n",
            "    (nn_decoder): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (1): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (2): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (3): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (4): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (5): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=6656, bias=True)\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            ")\n",
            "2022-05-06 15:16:22 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-05-06 15:16:22 | INFO | fairseq_cli.train | model: NNTransformer\n",
            "2022-05-06 15:16:22 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-05-06 15:16:22 | INFO | fairseq_cli.train | num. shared model params: 54,880,768 (num. trained: 54,880,768)\n",
            "2022-05-06 15:16:22 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-05-06 15:16:22 | INFO | fairseq.data.data_utils | loaded 5,089 examples from: /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset/valid.en-vi.en\n",
            "2022-05-06 15:16:22 | INFO | fairseq.data.data_utils | loaded 5,089 examples from: /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset/valid.en-vi.vi\n",
            "2022-05-06 15:16:22 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset valid en-vi 5089 examples\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.0.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.0.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.1.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.1.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.1.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.2.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.2.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.2.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.3.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.3.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.3.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.4.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.4.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.4.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.5.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.5.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.5.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.self_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.self_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.self_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:16:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-05-06 15:16:22 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2022-05-06 15:16:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-05-06 15:16:22 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-05-06 15:16:22 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer/checkpoint_last.pt\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer/checkpoint_last.pt\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-05-06 15:16:22 | INFO | fairseq.data.data_utils | loaded 111,966 examples from: /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset/train.en-vi.en\n",
            "2022-05-06 15:16:22 | INFO | fairseq.data.data_utils | loaded 111,966 examples from: /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset/train.en-vi.vi\n",
            "2022-05-06 15:16:22 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset train en-vi 111966 examples\n",
            "2022-05-06 15:16:22 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2022-05-06 15:16:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 816\n",
            "epoch 001:   0% 0/816 [00:00<?, ?it/s]2022-05-06 15:16:22 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-05-06 15:16:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/content/fairseq/fairseq/utils.py:375: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n",
            "epoch 001: 100% 815/816 [05:13<00:00,  2.51it/s, loss=8.455, nll_loss=7.754, ppl=215.8, wps=9375, ups=2.57, wpb=3647.4, bsz=139.4, num_updates=800, lr=0.0001, gnorm=1.591, train_wall=39, gb_free=11.7, wall=307]2022-05-06 15:21:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/47 [00:00<?, ?it/s]\u001b[A2022-05-06 15:21:38 | INFO | fairseq.tasks.translation | example hypothesis: Đây là một cách khác.\n",
            "2022-05-06 15:21:38 | INFO | fairseq.tasks.translation | example reference: Đây là một mẩu than đá.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   2% 1/47 [00:03<02:22,  3.10s/it]\u001b[A2022-05-06 15:21:42 | INFO | fairseq.tasks.translation | example hypothesis: Và đây là điều đó.\n",
            "2022-05-06 15:21:42 | INFO | fairseq.tasks.translation | example reference: Và vẫn chưa ai làm điều đó cả.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   4% 2/47 [00:06<02:17,  3.05s/it]\u001b[A2022-05-06 15:21:45 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta có thể làm thế giới.\n",
            "2022-05-06 15:21:45 | INFO | fairseq.tasks.translation | example reference: Họ chỉ chờ đến ngày ra toà.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   6% 3/47 [00:09<02:17,  3.12s/it]\u001b[A2022-05-06 15:21:48 | INFO | fairseq.tasks.translation | example hypothesis: Và tôi nghĩ rằng tôi đã làm việc này.\n",
            "2022-05-06 15:21:48 | INFO | fairseq.tasks.translation | example reference: Tôi nghĩ đó một thử nghiệm thú vị.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   9% 4/47 [00:12<02:16,  3.18s/it]\u001b[A2022-05-06 15:21:51 | INFO | fairseq.tasks.translation | example hypothesis: Đây là một người khác.\n",
            "2022-05-06 15:21:51 | INFO | fairseq.tasks.translation | example reference: Một năm sau, anh ta vẫn bốc mùi.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  11% 5/47 [00:15<02:12,  3.15s/it]\u001b[A2022-05-06 15:21:54 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta không phải là một người khác.\n",
            "2022-05-06 15:21:54 | INFO | fairseq.tasks.translation | example reference: Đó là một trật tự lớn nhỏ, lên xuống.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  13% 6/47 [00:18<02:09,  3.16s/it]\u001b[A2022-05-06 15:21:58 | INFO | fairseq.tasks.translation | example hypothesis: Và chúng tôi nói, \"Và bạn nói,\"\n",
            "2022-05-06 15:21:58 | INFO | fairseq.tasks.translation | example reference: rồi nói, \"Đi đi! Người tiếp theo!\"\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  15% 7/47 [00:22<02:07,  3.19s/it]\u001b[A2022-05-06 15:22:01 | INFO | fairseq.tasks.translation | example hypothesis: Và chúng ta có thể làm việc đó.\n",
            "2022-05-06 15:22:01 | INFO | fairseq.tasks.translation | example reference: Và hoạt động của họ cần chú ý cao độ.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  17% 8/47 [00:25<02:02,  3.15s/it]\u001b[A2022-05-06 15:22:04 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta có thể làm thế nào đó.\n",
            "2022-05-06 15:22:04 | INFO | fairseq.tasks.translation | example reference: Họ quay lại kiện thêm nhiều vụ vi phạm bản quyền hơn nữa.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  19% 9/47 [00:28<01:59,  3.16s/it]\u001b[A2022-05-06 15:22:07 | INFO | fairseq.tasks.translation | example hypothesis: Và tôi nghĩ rằng bạn có thể làm việc này.\n",
            "2022-05-06 15:22:07 | INFO | fairseq.tasks.translation | example reference: Tôi sẽ chỉ bạn một mánh khoé này, nếu bạn muốn chơi lại một lần nữa.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  21% 10/47 [00:31<01:57,  3.19s/it]\u001b[A2022-05-06 15:22:11 | INFO | fairseq.tasks.translation | example hypothesis: Và tôi muốn nói rằng bạn có thể thấy rằng bạn có thể làm việc này.\n",
            "2022-05-06 15:22:11 | INFO | fairseq.tasks.translation | example reference: Bây giờ tôi sẽ chia sẻ với các bạn quy trình bốn bước để tạo ra những vật liệu này.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  23% 11/47 [00:35<01:58,  3.28s/it]\u001b[A2022-05-06 15:22:14 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta có thể làm việc làm việc này.\n",
            "2022-05-06 15:22:14 | INFO | fairseq.tasks.translation | example reference: Sức mạnh của đất mẹ là một phần quan trọng giúp nỗi đau của họ dường như được giảm nhẹ.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  26% 12/47 [00:38<01:55,  3.31s/it]\u001b[A2022-05-06 15:22:17 | INFO | fairseq.tasks.translation | example hypothesis: Đây là một cách mà chúng ta có thể làm việc này.\n",
            "2022-05-06 15:22:17 | INFO | fairseq.tasks.translation | example reference: Công việc của tôi xoay quanh những hành vi mà chúng ta thực hiện vô thức ở mức độ tập thể.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  28% 13/47 [00:41<01:53,  3.35s/it]\u001b[A2022-05-06 15:22:21 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta có thể làm gì? Chúng ta có thể làm gì?\n",
            "2022-05-06 15:22:21 | INFO | fairseq.tasks.translation | example reference: Chúng ta thật sự nên hỏi rằng: Liệu thế giới có thể chịu đựng được số lượng xe hơi đó không?\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  30% 14/47 [00:45<01:52,  3.42s/it]\u001b[A2022-05-06 15:22:25 | INFO | fairseq.tasks.translation | example hypothesis: Tôi muốn nói rằng tôi có thể làm việc này.\n",
            "2022-05-06 15:22:25 | INFO | fairseq.tasks.translation | example reference: Tôi có thể làm ví dụ bằng giọng mình và chơi lại chúng chỉ bằng cách chạm vào mấy cái bảng này đây\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  32% 15/47 [00:49<01:55,  3.61s/it]\u001b[A2022-05-06 15:22:29 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta có thể làm điều đó là điều đó.\n",
            "2022-05-06 15:22:29 | INFO | fairseq.tasks.translation | example reference: Các phần tử khí bị đẩy ra khỏi mặt ấm đẩy ra xa với vận tốc tăng thêm bởi vì nó ấm.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  34% 16/47 [00:53<01:57,  3.80s/it]\u001b[A2022-05-06 15:22:33 | INFO | fairseq.tasks.translation | example hypothesis: Và đây là một người khác, chúng ta có thể làm việc này.\n",
            "2022-05-06 15:22:33 | INFO | fairseq.tasks.translation | example reference: Và truyền thuyết thứ tư là, ngủ sớm sẽ dậy sớm giúp ta khoẻ mạnh, giàu có và khôn ngoan.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  36% 17/47 [00:57<01:57,  3.92s/it]\u001b[A2022-05-06 15:22:38 | INFO | fairseq.tasks.translation | example hypothesis: Tôi muốn nói: Tôi muốn nói rằng: Tôi không phải không phải là một người.\n",
            "2022-05-06 15:22:38 | INFO | fairseq.tasks.translation | example reference: Shaffi Mather: Tôi chỉ cố gắng vượt qua những ngày đầu tiên khi tôi còn chưa bị loại bỏ.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  38% 18/47 [01:02<01:57,  4.05s/it]\u001b[A2022-05-06 15:22:42 | INFO | fairseq.tasks.translation | example hypothesis: Và chúng tôi nghĩ rằng chúng ta có thể làm việc này, và chúng ta có thể làm thế giới.\n",
            "2022-05-06 15:22:42 | INFO | fairseq.tasks.translation | example reference: Tiếp theo, tôi sẽ lướt nhanh hơn qua vài phần não chuyên biệt khác mà chúng tôi và những người khác đã tìm ra.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  40% 19/47 [01:06<01:57,  4.21s/it]\u001b[A2022-05-06 15:22:47 | INFO | fairseq.tasks.translation | example hypothesis: Vì vậy, chúng ta có thể làm việc này, và chúng ta có thể thấy rằng chúng ta có thể làm việc này.\n",
            "2022-05-06 15:22:47 | INFO | fairseq.tasks.translation | example reference: Giờ nếu chúng ta tháo chuỗi xoắn kép và mở tách hai chuỗi ra, chúng ta sẽ nhìn thấy chúng giống như hàm răng.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  43% 20/47 [01:11<01:54,  4.25s/it]\u001b[A2022-05-06 15:22:51 | INFO | fairseq.tasks.translation | example hypothesis: Và tôi muốn nói rằng, tôi muốn nói rằng tôi muốn nói rằng, nhưng tôi muốn nói rằng tôi muốn nói với những người khác.\n",
            "2022-05-06 15:22:51 | INFO | fairseq.tasks.translation | example reference: Và tôi đã nghĩ rằng khi tôi lại gần tôi sẽ nhìn thấy được chi tiết của từng người nhìn thấy được quần áo họ và vân vân.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  45% 21/47 [01:15<01:51,  4.28s/it]\u001b[A2022-05-06 15:22:56 | INFO | fairseq.tasks.translation | example hypothesis: Và chúng ta có thể làm việc này, và chúng ta có thể làm thế giới, nhưng chúng ta có thể làm thế giới.\n",
            "2022-05-06 15:22:56 | INFO | fairseq.tasks.translation | example reference: Bạn có thể thấy, đây là Sao Thổ, đây là Sao Mộc và trong lúc chúng ta dừng ở đây, tôi muốn chỉ ra một điều\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  47% 22/47 [01:20<01:50,  4.40s/it]\u001b[A2022-05-06 15:23:01 | INFO | fairseq.tasks.translation | example hypothesis: Và đây là điều này, tôi không phải là điều này.\n",
            "2022-05-06 15:23:01 | INFO | fairseq.tasks.translation | example reference: Và nó có hình dáng chiếc sừng trông rất buồn cười, mà cho tới nay tôi được biết đây là cuốn sách dạy nấu đầu tiên làm điều này.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  49% 23/47 [01:25<01:51,  4.66s/it]\u001b[A2022-05-06 15:23:06 | INFO | fairseq.tasks.translation | example hypothesis: Và đây là một người khác biệt là một người khác.\n",
            "2022-05-06 15:23:06 | INFO | fairseq.tasks.translation | example reference: Và nó là sản phẩm của một quá trình lịch sử phức tạp đã phát triển cùng với sự nổi lên của Hồi giáo bảo thủ kể từ cuối thập niên 1970.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  51% 24/47 [01:30<01:51,  4.85s/it]\u001b[A2022-05-06 15:23:11 | INFO | fairseq.tasks.translation | example hypothesis: Vì vậy, chúng ta có thể làm việc này, chúng ta có thể làm thế giới, nhưng chúng ta có thể làm thế giới.\n",
            "2022-05-06 15:23:11 | INFO | fairseq.tasks.translation | example reference: Con quạ đó tạm thời được độc quyền ăn lạc cho tới khi bạn nó tìm ra làm thế như thế nào, và thế là xong.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  53% 25/47 [01:35<01:47,  4.90s/it]\u001b[A2022-05-06 15:23:17 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta có thể làm việc này, nhưng nó không phải là một người, nhưng nó không phải là một cái gì đó.\n",
            "2022-05-06 15:23:17 | INFO | fairseq.tasks.translation | example reference: Ông viết rằng, khi đang ở trong trại, ông có thể đoán trước người nào sẽ được thả, người nào sẽ ổn và người nào không.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  55% 26/47 [01:41<01:44,  4.99s/it]\u001b[A2022-05-06 15:23:22 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta có thể nói rằng những người trong những người khác.\n",
            "2022-05-06 15:23:22 | INFO | fairseq.tasks.translation | example reference: Tổ Chức Y Tế Thế Giới ước tính -- họ đưa ra rất nhiều ước tính về số người cần đeo kính -- ước đoán thấp nhất là 150 triệu người.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  57% 27/47 [01:46<01:42,  5.11s/it]\u001b[A2022-05-06 15:23:27 | INFO | fairseq.tasks.translation | example hypothesis: Nếu bạn có thể thấy rằng, và tôi có thể làm việc này, và tôi có thể làm việc đó là điều đó là một cái gì đó.\n",
            "2022-05-06 15:23:27 | INFO | fairseq.tasks.translation | example reference: Giờ tôi hiểu rằng, biệt giam là một trong những nơi bất nhân và dã man nhất bạn có thể rơi vào, nhưng ở đó tôi đã tìm thấy chính mình.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  60% 28/47 [01:51<01:35,  5.05s/it]\u001b[A2022-05-06 15:23:32 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta có thể tìm thấy những người khác nhau, và những người khác nhau.\n",
            "2022-05-06 15:23:32 | INFO | fairseq.tasks.translation | example reference: Như bồi thẩm đoàn những người đã tuyên án những người vô tội đó và bồi thẩm đoàn đã kết án Titus, nhiều người tin rằng trí nhớ làm việc như một chiếc máy ghi hình.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  62% 29/47 [01:57<01:34,  5.24s/it]\u001b[A2022-05-06 15:23:38 | INFO | fairseq.tasks.translation | example hypothesis: Nếu bạn có thể nói rằng bạn có thể thấy rằng bạn có thể thấy rằng bạn có thể thấy rằng bạn có thể thấy rằng bạn có thể thấy rằng nó.\n",
            "2022-05-06 15:23:38 | INFO | fairseq.tasks.translation | example reference: Nếu tôi nói với bạn có 1 đợt dịch hạch sẽ giết 15,000 người Mỹ năm tới, bạn sẽ hốt hoảng nếu bạn không phát hiện ra đó là bệnh cúm.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  64% 30/47 [02:02<01:31,  5.41s/it]\u001b[A2022-05-06 15:23:45 | INFO | fairseq.tasks.translation | example hypothesis: Vì vậy, chúng tôi có thể làm việc này, nhưng không phải là những người khác nhau, nhưng tôi có thể làm thế giới.\n",
            "2022-05-06 15:23:45 | INFO | fairseq.tasks.translation | example reference: Những nhà hoạt động mà tôi phỏng vấn không có điểm chung nào trừ một điều họ luôn nhắc tới mẹ họ như hình bóng to lớn ảnh hưởng sâu sắc đến họ.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  66% 31/47 [02:09<01:30,  5.67s/it]\u001b[A2022-05-06 15:23:50 | INFO | fairseq.tasks.translation | example hypothesis: Đây là một người khác, những người khác nhau, một người khác, nhưng nó là một người khác nhau.\n",
            "2022-05-06 15:23:50 | INFO | fairseq.tasks.translation | example reference: Như chính phủ I-ran phát hiện ra thông qua một chuỗi các công ty bình phong, công ty nặc danh sở hữu một toà nhà ngay trung tâm của Manhattan trên Đại lộ 5, bất chấp các cuộc trừng phạt của Mỹ.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  68% 32/47 [02:15<01:25,  5.73s/it]\u001b[A2022-05-06 15:23:57 | INFO | fairseq.tasks.translation | example hypothesis: Và chúng ta có thể làm việc này, nhưng chúng ta có thể làm việc này, nhưng chúng ta có thể làm thế giới.\n",
            "2022-05-06 15:23:57 | INFO | fairseq.tasks.translation | example reference: Và vào ngày 21 tháng 9 năm nay, chúng tôi sẽ khởi động chiến dịch tại O2 Arena tiến lên vì hoạt động đó, để cố gắng tạo ra kỉ lục lớn nhất về sự loại bỏ thù địch.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  70% 33/47 [02:21<01:21,  5.84s/it]\u001b[A2022-05-06 15:24:03 | INFO | fairseq.tasks.translation | example hypothesis: Và chúng ta có thể làm việc đó là một cái mà chúng ta có thể làm việc này.\n",
            "2022-05-06 15:24:03 | INFO | fairseq.tasks.translation | example reference: Chúng tôi muốn tìm ra liệu có sự khác biệt nhỏ nào trong các cộng đồng về lượng tương tác mỗi con kiến cần để sẵn sàng ra ngoài kiếm ăn, khi cộng đồng ấy ít đi kiếm ăn.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  72% 34/47 [02:27<01:17,  5.93s/it]\u001b[A2022-05-06 15:24:10 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta có thể thấy những người khác nhau, và những người khác nhau, và những người khác nhau, và những người khác nhau.\n",
            "2022-05-06 15:24:10 | INFO | fairseq.tasks.translation | example reference: Loài tanager ở bờ Đông nước Mỹ, khi rừng có phần rậm rạp hơn, có tiếng hót khác so với loài tanUNKNOWNTOKENINREF ở phía bên kia, phía tây vậy là chúng rất khác nhau.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  74% 35/47 [02:34<01:16,  6.35s/it]\u001b[A2022-05-06 15:24:16 | INFO | fairseq.tasks.translation | example hypothesis: Chúng tôi muốn nói rằng những người khác, nhưng tôi không phải là một cái gì đó là một người khác nhau, nhưng tôi đã nói về những người khác.\n",
            "2022-05-06 15:24:16 | INFO | fairseq.tasks.translation | example reference: Do đó khi Sở công viên liên hệ với tôi về chương trình cấp hạt giống trị giá $10: 00, một chương trình nhằm giúp phát triển những dự án đê điều, tôi nghĩ ngay rằng ý đồ đó tốt, nhưng có chút ngây thơ.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  77% 36/47 [02:41<01:10,  6.39s/it]\u001b[A2022-05-06 15:24:24 | INFO | fairseq.tasks.translation | example hypothesis: Vì vậy, chúng ta có thể làm việc làm thế giới, chúng ta có thể làm việc làm thế giới, chúng ta có thể làm thế giới.\n",
            "2022-05-06 15:24:24 | INFO | fairseq.tasks.translation | example reference: ngay bây giờ, thế giới có thể sản xuất khoảng 350 triệu liều vắc xin cúm cho 3 biến thể và chúng ta có thể có tới 1.2 tỷ liều nếu chúng ta muốn đặt mục tiêu cho một biến thể đơn như cúm lợn\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  79% 37/47 [02:48<01:07,  6.74s/it]\u001b[A2022-05-06 15:24:32 | INFO | fairseq.tasks.translation | example hypothesis: Vì vậy, chúng ta có thể thấy những người khác nhau, nhưng chúng ta có thể thấy rằng những người khác nhau, nhưng chúng ta có thể thấy rằng những người khác nhau, nhưng chúng ta có thể làm thế giới.\n",
            "2022-05-06 15:24:32 | INFO | fairseq.tasks.translation | example reference: Như vậy, tuổi thọ trung bình ở Mỹ và Anh là 78.1 tuổi, nhưng chúng ta biết được từ hơn 1000 cuộc nghiên cứu khoa học được kiểm tra kỹ càng rằng bạn có thể sống thêm 10 năm bằng cách nâng cao bốn loại sức bật này.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  81% 38/47 [02:56<01:03,  7.06s/it]\u001b[A2022-05-06 15:24:39 | INFO | fairseq.tasks.translation | example hypothesis: Và chúng ta có thể thấy rằng chúng ta có thể làm việc này, nhưng chúng ta có thể thấy rằng chúng ta có thể làm thế giới.\n",
            "2022-05-06 15:24:39 | INFO | fairseq.tasks.translation | example reference: Sẽ không thể nhìn hay cảm nhận bất cứ thứ gì giống như chúng ta thấy khi chúng ta nhìn một bông hoa. Vậy nếu bạn nhìn bông hoa ở đây, và bạn là một con côn trùng bé xíu, bạn ở trên bề mặt của bông hoa đó, địa hình cũng giống như vậy,\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  83% 39/47 [03:03<00:56,  7.05s/it]\u001b[A2022-05-06 15:24:47 | INFO | fairseq.tasks.translation | example hypothesis: Vì vậy, chúng ta có thể làm việc này, và tôi đã nói rằng những người khác nhau, và nó là một người khác nhau, và những người khác, nhưng tôi đã làm việc đó là một người khác.\n",
            "2022-05-06 15:24:47 | INFO | fairseq.tasks.translation | example reference: Vậy mà, nếu tôi đã học được điều gì trong gần 12 năm nay kéo những vật nặng quanh những nơi lạnh giá, đó là cảm hứng thực sự, và sự trưởng thành chỉ đến từ khó khăn và thử thách, từ việc rời khỏi những cái tiện nghi và thân thuộc và bước chân vào những nơi chưa biết đến.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  85% 40/47 [03:11<00:51,  7.42s/it]\u001b[A2022-05-06 15:24:56 | INFO | fairseq.tasks.translation | example hypothesis: Vì vậy, chúng ta có thể tìm thấy những người khác nhau, nhưng chúng ta có thể làm việc làm thế giới.\n",
            "2022-05-06 15:24:56 | INFO | fairseq.tasks.translation | example reference: Những người máy sẽ không thế hết công việc của chúng ta trong một vài năm tới vậy nên cuốn sách chiến lược kinh tế 101 sẽ vẫn có hiệu lực: Khuyến khích kinh doanh, giàm gấp đôi về cơ sở hạ tầng và đảm bảo tạo ra những người lao động bước ra từ hệ thống giáo dục với những kỹ năng phù hợp.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  87% 41/47 [03:20<00:47,  7.89s/it]\u001b[A2022-05-06 15:25:06 | INFO | fairseq.tasks.translation | example hypothesis: Vì vậy, chúng ta có thể tìm thấy những người khác nhau, nhưng nó có thể tìm thấy những người khác nhau, nhưng bạn có thể tìm thấy những người khác nhau.\n",
            "2022-05-06 15:25:06 | INFO | fairseq.tasks.translation | example reference: Chúng ta đều được đặt trong những mạng xã hội gồn bạn bè, gia đình, đồng nghiệp và hơn thế nữa. Nicholas Christakis theo dõi cách những đặc tính đa dạng -- từ hạnh phúc cho đến nạn béo phì -- có thể lan truyền từ người này sang người khác, cho thấy vị trí của bạn trong mạng lưới có thể ảnh hưởng tới cuộc sống của chính bạn theo những cách mà bạn thậm chí không biết.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  89% 42/47 [03:30<00:42,  8.59s/it]\u001b[A2022-05-06 15:25:19 | INFO | fairseq.tasks.translation | example hypothesis: Chúng ta có thể nói rằng những người khác, nhưng không phải là một người khác nhau, nhưng không phải là một người khác nhau, nhưng không phải là một người khác nhau.\n",
            "2022-05-06 15:25:19 | INFO | fairseq.tasks.translation | example reference: Ngày nay, khi bạn suy nghĩ một sự thật rằng, về mặt lịch sử trung tâm R & amp; D của các công ty đa quốc gia luôn luôn đặt ở văn phòng đầu não, hoặc tại các quốc gia gốc của công ty đa quốc gia đó, việc có được 750 trung tâm R & amp; D của các tập đoàn đa quốc gia tại Ấn Độ đó thực sự là một con số ấn tượng.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  91% 43/47 [03:43<00:39,  9.76s/it]\u001b[A2022-05-06 15:25:35 | INFO | fairseq.tasks.translation | example hypothesis: Vì vậy, chúng ta có thể tìm thấy những người khác, nhưng chúng ta có thể thấy những người khác nhau, nhưng họ có thể thấy những người khác nhau, nhưng họ có thể làm việc này, và họ có thể thấy những người khác nhau.\n",
            "2022-05-06 15:25:35 | INFO | fairseq.tasks.translation | example reference: Ngày nay, thuốc kháng sinh được sử dụng cho những bệnh nhân như thế này, nhưng cũng được sử dụng phí phạm cho những trường hợp rất nhẹ, như chữa cho người bị cảm, cúm, những bệnh không có phản hồi với thuốc kháng sinh. Và thuốc kháng sinh cũng được sử dụng với lượng lớn, không mang tính trị liệu, với nồng độ thấp, để kích thích sinh trưởng ở gà và lợn.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  94% 44/47 [03:59<00:35, 11.78s/it]\u001b[A2022-05-06 15:25:51 | INFO | fairseq.tasks.translation | example hypothesis: Và đây là một người, tôi muốn nói về những người khác, nhưng tôi đã nói rằng, nhưng tôi muốn nói về những người khác, và những người khác, nhưng tôi đã nói về những người khác, nhưng không phải là một cái gì đó.\n",
            "2022-05-06 15:25:51 | INFO | fairseq.tasks.translation | example reference: Và để nhấn mạnh điểm này, tôi đã bơi phía dưới con tầu đánh bắt tôm và chụp bức ảnh này của một gã đang xúc những sinh vật bị bắt nhầm này đổ ra biển như là rác và chụp được dòng thác của cái chết, Bạn biết đấy, các sinh vật thuộc họ cá đuối, cá bơn, cá nóc, mà chỉ một giờ trước đây, còn đang ở dưới đáy của đại dương, còn sống, nhưng giờ đây bị ném trở lại như là rác\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  96% 45/47 [04:16<00:26, 13.08s/it]\u001b[A2022-05-06 15:26:22 | INFO | fairseq.tasks.translation | example hypothesis: Tôi muốn nói rằng, \"Tôi muốn nói rằng,\" Tôi muốn nói rằng, \"Tôi muốn nói rằng,\" Tôi muốn nói, \"Tôi muốn nói,\" Tôi muốn nói rằng \"Tôi muốn nói rằng,\"\n",
            "2022-05-06 15:26:22 | INFO | fairseq.tasks.translation | example reference: Nhưng cũng để lại trong tôi tình thế lưỡng nan. Tiếp tục theo nghiệp viết lách, cố gắng tìm ra cách viết lại một quyển sách mà mọi người đều hài lòng, vì tôi biết rằng những người say mê \"Ăn, Cầu nguyện, Yêu\" sẽ vô cùng thất vọng với bất kỳ tác phẩm nào của tôi sau đó không phải là \"Ăn, Cầu nguyện, Yêu\". và những người ghét \"Ăn, Cầu nguyện, Yêu\" cũng sẽ rất thất vọng về những tác phẩm sau đó bởi chúng là bằng chứng rằng tôi vẫn tồn tại.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  98% 46/47 [04:46<00:18, 18.24s/it]\u001b[A2022-05-06 15:26:24 | INFO | fairseq.tasks.translation | example hypothesis: Vì vậy, chúng tôi đã nói rằng những người, nhưng không phải là một người khác, nhưng không phải là một người khác, nhưng nó, nhưng không phải là một con người, và những người, nhưng tôi đã nói về những người khác, và những người khác.\n",
            "2022-05-06 15:26:24 | INFO | fairseq.tasks.translation | example reference: Và sự nổi dậy của hàng loạt phong trào như Chúng tôi là một phần trăm Thế hệ tài nguyên (Resource Generation hay Sự giàu có cho lợi ích chung trong đó những thành viên có đặc quyền nhất trong dân số thành viên của một phần trăm những người giàu có đang dùng tài sản kinh tế của mình, dù trưởng thành hay còn trẻ, đó là điều làm tôi bất ngờ nhất, dùng những đặc quyền của họ những tài sản kinh tế của họ để chống lại sự bất bình đẳng bằng cách ủng hộ những chính sách xã hội những thay đổi về giá trị xã hội và thay đổi trong hành vi con người cho dù chúng chống lại lợi ích kinh tế của chính họ nhưng trên hết sẽ khôi phục Giấc mơ Mỹ\n",
            "\n",
            "epoch 001 | valid on 'valid' subset: 100% 47/47 [04:48<00:00, 13.46s/it]\u001b[A\n",
            "                                                                        \u001b[A2022-05-06 15:26:24 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.174 | nll_loss 7.395 | ppl 168.29 | bleu 1.82 | wps 470.3 | wpb 2916.9 | bsz 108.3 | num_updates 816\n",
            "2022-05-06 15:26:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 816 updates\n",
            "2022-05-06 15:26:24 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer/checkpoint1.pt\n",
            "2022-05-06 15:26:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer/checkpoint1.pt\n",
            "2022-05-06 15:26:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer/checkpoint1.pt (epoch 1 @ 816 updates, score 1.82) (writing took 9.942253876999985 seconds)\n",
            "2022-05-06 15:26:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-05-06 15:26:34 | INFO | train | epoch 001 | loss 9.714 | nll_loss 9.235 | ppl 602.44 | wps 4850.9 | ups 1.33 | wpb 3641.2 | bsz 137.2 | num_updates 816 | lr 0.000102 | gnorm 1.487 | train_wall 311 | gb_free 11.6 | wall 612\n",
            "2022-05-06 15:26:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 816\n",
            "epoch 002:   0% 0/816 [00:00<?, ?it/s]2022-05-06 15:26:34 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-05-06 15:26:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/bin/fairseq-train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 316, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 1058, in train_step\n",
            "    gb_used = torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py\", line 341, in max_memory_allocated\n",
            "    return memory_stats(device=device).get(\"allocated_bytes.all.peak\", 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py\", line 198, in memory_stats\n",
            "    stats = memory_stats_as_nested_dict(device=device)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py\", line 210, in memory_stats_as_nested_dict\n",
            "    return torch._C._cuda_memoryStats(device)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# Use MyTransformer to train en-vi data\n",
        "# Train the model, please change --max-epoch depends whether you use gpu/cpu\n",
        "# see stop-time-hours\n",
        "%env CUDA_VISIBLE_DEVICES=0 \n",
        "!fairseq-train \\\n",
        "    /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset \\\n",
        "    --arch nntransformer_default \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-epoch 100 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir /content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer \\\n",
        "    --user-dir /content/mymodel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training custom proposal 4 transformer"
      ],
      "metadata": {
        "id": "FPIFPwDde0Xk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78bcecd-706d-42a5-a66f-850f44b9957d",
        "id": "fEFW9OOkfLiT"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "2022-05-07 14:41:31 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-05-07 14:41:34 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/content/mymodel', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-proposal-4', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='proposal4_default', adam_betas='(0.9, 0.98)', adam_eps=1e-08, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='proposal4_default', azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', device_id=0, dim_feedforward=2048, dim_fuzzy=128, dim_model=512, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=100, max_source_positions=1024, max_src_len=16378, max_target_positions=1024, max_tgt_len=16378, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_head=8, num_layer=6, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-proposal-4', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/content/mymodel', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-05-07 14:41:36 | INFO | fairseq.tasks.translation | [en] dictionary: 7656 types\n",
            "2022-05-07 14:41:36 | INFO | fairseq.tasks.translation | [vi] dictionary: 6656 types\n"
          ]
        }
      ],
      "source": [
        "# Train the model, please change --max-epoch depends whether you use gpu/cpu\n",
        "# see stop-time-hours\n",
        "%env CUDA_VISIBLE_DEVICES=0 \n",
        "!fairseq-train \\\n",
        "    /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset \\\n",
        "    --arch proposal4_default \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-epoch 100 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir /content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-proposal-4 \\\n",
        "    --user-dir /content/mymodel\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_OBkDDiNQPuD",
        "CBYOOYRFbojI",
        "uuVoveca1ATX",
        "YYLNU2e0Ow0d",
        "KsmOKipnchcY",
        "N0Wbr-bJW4-r",
        "icDOr3H9HlHY",
        "4Wp9VGKqQVPb",
        "7DGeQy3fetQB"
      ],
      "name": "running fairseq",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}