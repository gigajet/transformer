{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gigajet/transformer/blob/master/running_fairseq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OBkDDiNQPuD"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C-uhHC2jy04F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec14efca-0eb9-4d4f-c89a-376906a3943f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Google Colab doesn't support `ln`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaNm6BzuuVW8",
        "outputId": "cb865ccb-ffbe-444a-c126-48c00255a7ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 31258, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 31258 (delta 10), reused 17 (delta 8), pack-reused 31225\u001b[K\n",
            "Receiving objects: 100% (31258/31258), 21.51 MiB | 31.29 MiB/s, done.\n",
            "Resolving deltas: 100% (23060/23060), done.\n",
            "/content/fairseq\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e71c4d0) (4.64.0)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[K     |████████████████████████████████| 236 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e71c4d0) (2019.12.20)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 64.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e71c4d0) (1.21.6)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e71c4d0) (1.15.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e71c4d0) (0.29.28)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 12.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e71c4d0) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+e71c4d0) (0.11.0+cu113)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 72.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+e71c4d0) (5.7.1)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 65.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+e71c4d0) (4.2.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+e71c4d0) (0.8.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+e71c4d0) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+e71c4d0) (3.8.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=550768e49dc31e6b1180e5465f5d56e92133ca806540873df688a1e992249cd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.5.0 colorama-0.4.4 fairseq hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.4.0 sacrebleu-2.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install fairseq\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd fairseq\n",
        "!pip install --editable ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMStVQCYu-nN",
        "outputId": "660d18b6-90b2-43d7-ce71-a2f1058c2ecb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastBPE\n",
            "  Downloading fastBPE-0.1.0.tar.gz (35 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting subword_nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Building wheels for collected packages: fastBPE, sacremoses\n",
            "  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastBPE: filename=fastBPE-0.1.0-cp37-cp37m-linux_x86_64.whl size=483188 sha256=61ef3682fc23cf9be18ff16594aec0dab98b000f80ca41fce0af805ed5f4db50\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/d4/0e/0d317a65f77d3f8049fedd8a2ee0519164cf3e6bd77ef886f1\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=9cad2f35f4a217b0b7f48e509fa00d7fce5ad11435c1872385b0913ac1b593de\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built fastBPE sacremoses\n",
            "Installing collected packages: mock, subword-nmt, sacremoses, fastBPE\n",
            "Successfully installed fastBPE-0.1.0 mock-4.0.3 sacremoses-0.0.53 subword-nmt-0.3.8\n"
          ]
        }
      ],
      "source": [
        "# Dependencies for preprocessing\n",
        "!pip install fastBPE sacremoses subword_nmt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CD6pa7A0CgRX"
      },
      "outputs": [],
      "source": [
        "# Run this if you get error \"importlib_metadata.PackageNotFoundError: No package metadata was found for fairseq\" in next cell\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBYOOYRFbojI"
      },
      "source": [
        "## Verify that fairseq is now usable (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "cAecsqc7vGTE",
        "outputId": "3cdf48be-1ce8-482f-b696-0e0e9fcc1302"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-02 04:29:06 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "Downloading: \"https://github.com/pytorch/fairseq/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n",
            "2022-05-02 04:29:10 | INFO | fairseq.file_utils | https://dl.fbaipublicfiles.com/fairseq/models/wmt16.en-de.joined-dict.transformer.tar.bz2 not found in cache, downloading to /tmp/tmpy9uuosu6\n",
            "  7%|▋         | 150277120/2193287384 [00:05<01:50, 18416384.24B/s]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-08ea59ea7e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Note: WMT'19 models use fastBPE instead of subword_nmt, see instructions below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt16.en-de',\n\u001b[0;32m---> 12\u001b[0;31m                        tokenizer='moses', bpe='subword_nmt')\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0men2de\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# disable dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mrepo_or_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_cache_or_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhubconf_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fairseq/fairseq/models/fairseq_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_name_or_path, checkpoint_file, data_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mdata_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0marchive_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         )\n\u001b[1;32m    274\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fairseq/fairseq/hub_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name_or_path, checkpoint_file, data_name_or_path, archive_map, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_archive_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# convenience hack for loading data and BPE codes from model archive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fairseq/fairseq/file_utils.py\u001b[0m in \u001b[0;36mload_archive_file\u001b[0;34m(archive_file)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# redirect to the cache, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mresolved_archive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         logger.info(\n",
            "\u001b[0;32m/content/fairseq/fairseq/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# File, and it exists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fairseq/fairseq/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0ms3_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# we are copying the file before closing it, so flush to avoid truncation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fairseq/fairseq/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontent_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;31m# Amount is not given (unbounded read) so we must check self.length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# This cell is for verifying the install\n",
        "# Interactive translation\n",
        "import torch\n",
        "import fairseq\n",
        "\n",
        "# List available models\n",
        "torch.hub.list('pytorch/fairseq')  # [..., 'transformer.wmt16.en-de', ... ]\n",
        "\n",
        "# Load a transformer trained on WMT'16 En-De\n",
        "# Note: WMT'19 models use fastBPE instead of subword_nmt, see instructions below\n",
        "en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt16.en-de',\n",
        "                       tokenizer='moses', bpe='subword_nmt')\n",
        "en2de.eval()  # disable dropout\n",
        "\n",
        "# The underlying model is available under the *models* attribute\n",
        "assert isinstance(en2de.models[0], fairseq.models.transformer.TransformerModel)\n",
        "\n",
        "# Move model to GPU for faster translation\n",
        "# en2de.cuda()\n",
        "\n",
        "# Translate a sentence\n",
        "en2de.translate('Hello world!')\n",
        "# 'Hallo Welt!'\n",
        "\n",
        "# Batched translation\n",
        "en2de.translate(['Hello world!', 'The cat sat on the mat.'])\n",
        "# ['Hallo Welt!', 'Die Katze saß auf der Matte.']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuVoveca1ATX"
      },
      "source": [
        "## Prepare & Train IWSLT'14 German to English (Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9uIZOA0-03u",
        "outputId": "2ccdda54-e869-4bca-a1b9-643a7abbc499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/fairseq/examples/translation\n",
            "Cloning Moses github repository (for tokenization scripts)...\n",
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 148090, done.\u001b[K\n",
            "remote: Counting objects: 100% (518/518), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "^C\n",
            "/content/fairseq\n"
          ]
        }
      ],
      "source": [
        "# Download and prepare the data\n",
        "%cd examples/translation/\n",
        "!bash prepare-iwslt14.sh\n",
        "%cd ../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01aLMot8vkby",
        "outputId": "9b37e8e5-642f-4acf-ade8-a003a8f293b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: TEXT=examples/translation/iwslt14.tokenized.de-en\n",
            "/bin/bash: fairseq-preprocess: command not found\n"
          ]
        }
      ],
      "source": [
        "# Preprocess/binarize the data\n",
        "# Look below on how to set environment variable in GG Colab\n",
        "%env TEXT=examples/translation/iwslt14.tokenized.de-en\n",
        "!fairseq-preprocess --source-lang de --target-lang en \\\n",
        "    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
        "    --destdir /content/drive/MyDrive/translation/iwslt14.tokenized.de-en \\\n",
        "    --workers 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6m9cR6h1HPM",
        "outputId": "398179fe-af5e-41a9-bd05-7c4a8b8808ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "/bin/bash: fairseq-train: command not found\n"
          ]
        }
      ],
      "source": [
        "# Train the model, please change --max-epoch depends whether you use gpu/cpu\n",
        "%env CUDA_VISIBLE_DEVICES=0 \n",
        "!fairseq-train \\\n",
        "    /content/drive/MyDrive/translation/iwslt14.tokenized.de-en \\\n",
        "    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-epoch 15 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIVXq97t1JnF",
        "outputId": "ba78b363-c3a9-461d-de5d-a3f97d791a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: fairseq-generate: command not found\n"
          ]
        }
      ],
      "source": [
        "# Evaluate our model\n",
        "!fairseq-generate /content/drive/MyDrive/translation/iwslt14.tokenized.de-en \\\n",
        "    --path /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/checkpoints/checkpoint_best.pt \\\n",
        "    --batch-size 128 --beam 5 --remove-bpe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYLNU2e0Ow0d"
      },
      "source": [
        "## Prepare IWSLT'15 English - Vietnamese data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RphxFkxj6wIW",
        "outputId": "68da7334-7d27-414d-839f-702ab9276398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/translation/en-vi.zip\n",
            "  inflating: en-vi/IWSLT15.TED.dev2010.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.dev2010.en-vi.vi  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2010.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2010.en-vi.vi  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2011.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2011.en-vi.vi  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2012.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2012.en-vi.vi  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2013.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2013.en-vi.vi  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2015.en-vi.en  \n",
            "  inflating: en-vi/IWSLT15.TED.tst2015.en-vi.vi  \n",
            "  inflating: en-vi/train.en          \n",
            "  inflating: en-vi/train.en-vi       \n",
            "  inflating: en-vi/train.tags.en-vi.clean.en  \n",
            "  inflating: en-vi/train.tags.en-vi.clean.vi  \n",
            "  inflating: en-vi/train.tags.en-vi.en  \n",
            "  inflating: en-vi/train.tags.en-vi.tok.en  \n",
            "  inflating: en-vi/train.tags.en-vi.tok.vi  \n",
            "  inflating: en-vi/train.tags.en-vi.vi  \n",
            "  inflating: en-vi/train.vi          \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/translation/en-vi.zip -d en-vi\n",
        "!cp /content/drive/MyDrive/translation/myprepare-iwslt15-en-vi.sh ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdtKEvLYcpP1",
        "outputId": "a34c47bb-1f8a-4354-ae95-acd51a06734b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning Moses github repository (for tokenization scripts)...\n",
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 148090, done.\u001b[K\n",
            "remote: Counting objects: 100% (518/518), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "remote: Total 148090 (delta 319), reused 443 (delta 292), pack-reused 147572\u001b[K\n",
            "Receiving objects: 100% (148090/148090), 129.87 MiB | 18.58 MiB/s, done.\n",
            "Resolving deltas: 100% (114345/114345), done.\n",
            "Cloning Subword NMT repository (for BPE pre-processing)...\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 590, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 590 (delta 3), reused 4 (delta 1), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (590/590), 245.03 KiB | 4.90 MiB/s, done.\n",
            "Resolving deltas: 100% (352/352), done.\n",
            "Creating backup of train.en train.vi\n",
            "creating train, valid, test...\n",
            "learn_bpe.py on en-vi/train.en-vi...\n",
            "100% 10000/10000 [00:12<00:00, 813.07it/s]\n",
            "apply_bpe.py to train.en...\n",
            "apply_bpe.py to valid.en...\n",
            "apply_bpe.py to test.en...\n",
            "apply_bpe.py to train.vi...\n",
            "apply_bpe.py to valid.vi...\n",
            "apply_bpe.py to test.vi...\n"
          ]
        }
      ],
      "source": [
        "!sh myprepare-iwslt15-en-vi.sh\n",
        "# After this, we have `train valid test` in iwslt15.tokenized.en-vi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLUymR6GeKvp",
        "outputId": "30c38f1e-9e09-40c8-a7fe-9067bf2483a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TEXT=iwslt15.tokenized.en-vi\n",
            "2022-05-06 15:02:56 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-05-06 15:02:56 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='/content/drive/MyDrive/iwslt15.tokenized.en-vi/dataset', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='vi', task='translation', tensorboard_logdir=None, testpref='iwslt15.tokenized.en-vi/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='iwslt15.tokenized.en-vi/train', use_plasma_view=False, user_dir=None, validpref='iwslt15.tokenized.en-vi/valid', wandb_project=None, workers=20)\n",
            "2022-05-06 15:03:11 | INFO | fairseq_cli.preprocess | [en] Dictionary: 7656 types\n",
            "2022-05-06 15:03:23 | INFO | fairseq_cli.preprocess | [en] iwslt15.tokenized.en-vi/train.en: 111966 sents, 2768010 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-06 15:03:23 | INFO | fairseq_cli.preprocess | [en] Dictionary: 7656 types\n",
            "2022-05-06 15:03:24 | INFO | fairseq_cli.preprocess | [en] iwslt15.tokenized.en-vi/valid.en: 5089 sents, 128011 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-06 15:03:24 | INFO | fairseq_cli.preprocess | [en] Dictionary: 7656 types\n",
            "2022-05-06 15:03:26 | INFO | fairseq_cli.preprocess | [en] iwslt15.tokenized.en-vi/test.en: 7446 sents, 176932 tokens, 0.00339% replaced (by <unk>)\n",
            "2022-05-06 15:03:26 | INFO | fairseq_cli.preprocess | [vi] Dictionary: 6656 types\n",
            "2022-05-06 15:03:44 | INFO | fairseq_cli.preprocess | [vi] iwslt15.tokenized.en-vi/train.vi: 111966 sents, 2971232 tokens, 0.0% replaced (by <unk>)\n",
            "2022-05-06 15:03:44 | INFO | fairseq_cli.preprocess | [vi] Dictionary: 6656 types\n",
            "2022-05-06 15:03:46 | INFO | fairseq_cli.preprocess | [vi] iwslt15.tokenized.en-vi/valid.vi: 5089 sents, 137095 tokens, 0.0277% replaced (by <unk>)\n",
            "2022-05-06 15:03:46 | INFO | fairseq_cli.preprocess | [vi] Dictionary: 6656 types\n",
            "2022-05-06 15:03:47 | INFO | fairseq_cli.preprocess | [vi] iwslt15.tokenized.en-vi/test.vi: 7446 sents, 197843 tokens, 0.16% replaced (by <unk>)\n",
            "2022-05-06 15:03:47 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /content/drive/MyDrive/iwslt15.tokenized.en-vi/dataset\n"
          ]
        }
      ],
      "source": [
        "# Preprocess/binarize the data\n",
        "# Look below on how to set environment variable in GG Colab\n",
        "%env TEXT=iwslt15.tokenized.en-vi\n",
        "!fairseq-preprocess --source-lang en --target-lang vi \\\n",
        "    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
        "    --destdir /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset \\\n",
        "    --workers 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsmOKipnchcY"
      },
      "source": [
        "## Train IWSLT'15 English-Vietnamese (Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXd1Mv0Q7Cez",
        "outputId": "353dcaef-9d27-439a-c4d9-771c89682cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "/bin/bash: fairseq-train: command not found\n"
          ]
        }
      ],
      "source": [
        "# arch transformer, ở đây dùng transformer_iwslt_de_en là dùng lại architecture của nó\n",
        "# Train the model, please change --max-epoch depends whether you use gpu/cpu\n",
        "# see stop-time-hours\n",
        "%env CUDA_VISIBLE_DEVICES=0 \n",
        "!fairseq-train \\\n",
        "    /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset \\\n",
        "    --arch transformer_iwslt_de_en --share-decoder-input-output-embed \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-epoch 36 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir checkpoints\n",
        "    # --save-dir /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tE1kX47f3YkB",
        "outputId": "1edfd8f9-9846-4711-edb4-043a0f6aa4a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: fairseq-generate: command not found\n"
          ]
        }
      ],
      "source": [
        "# Evaluate our model\n",
        "!fairseq-generate /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset \\\n",
        "    --path /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/checkpoints/checkpoint_best.pt \\\n",
        "    --batch-size 128 --beam 5 --remove-bpe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Wbr-bJW4-r"
      },
      "source": [
        "## Try to add FuzzyLayer to Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iagQ0QWRb4n7",
        "outputId": "70eb4f54-b48b-4bce-fdc5-a7a77ae583fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 35])\n",
            "tensor([[6.7112e-06, 6.8293e-04, 1.1360e-05, 1.4338e-05, 3.1253e-06, 4.4289e-04,\n",
            "         1.2499e-04, 2.6953e-04, 5.7319e-06, 3.1216e-05, 5.5655e-05, 3.2802e-04,\n",
            "         3.7528e-02, 1.1846e-05, 7.1205e-07, 2.0275e-06, 7.1357e-07, 4.7466e-04,\n",
            "         1.9231e-06, 2.3387e-03, 8.4445e-06, 1.2212e-04, 1.2321e-04, 4.9719e-04,\n",
            "         9.7817e-03, 7.2931e-03, 2.3511e-06, 5.0083e-02, 1.0146e-05, 1.0202e-06,\n",
            "         5.4978e-04, 8.7630e-04, 9.0879e-02, 2.8873e-08, 5.0999e-03],\n",
            "        [6.7112e-06, 6.8293e-04, 1.1360e-05, 1.4338e-05, 3.1253e-06, 4.4289e-04,\n",
            "         1.2499e-04, 2.6953e-04, 5.7319e-06, 3.1216e-05, 5.5655e-05, 3.2802e-04,\n",
            "         3.7528e-02, 1.1846e-05, 7.1205e-07, 2.0275e-06, 7.1357e-07, 4.7466e-04,\n",
            "         1.9231e-06, 2.3387e-03, 8.4445e-06, 1.2212e-04, 1.2321e-04, 4.9719e-04,\n",
            "         9.7817e-03, 7.2931e-03, 2.3511e-06, 5.0083e-02, 1.0146e-05, 1.0202e-06,\n",
            "         5.4978e-04, 8.7630e-04, 9.0879e-02, 2.8873e-08, 5.0999e-03],\n",
            "        [6.7112e-06, 6.8293e-04, 1.1360e-05, 1.4338e-05, 3.1253e-06, 4.4289e-04,\n",
            "         1.2499e-04, 2.6953e-04, 5.7319e-06, 3.1216e-05, 5.5655e-05, 3.2802e-04,\n",
            "         3.7528e-02, 1.1846e-05, 7.1205e-07, 2.0275e-06, 7.1357e-07, 4.7466e-04,\n",
            "         1.9231e-06, 2.3387e-03, 8.4445e-06, 1.2212e-04, 1.2321e-04, 4.9719e-04,\n",
            "         9.7817e-03, 7.2931e-03, 2.3511e-06, 5.0083e-02, 1.0146e-05, 1.0202e-06,\n",
            "         5.4978e-04, 8.7630e-04, 9.0879e-02, 2.8873e-08, 5.0999e-03]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# FuzzyLayer\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "\n",
        "class FuzzyLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim: int, **kwargs):\n",
        "        super(FuzzyLayer, self).__init__(**kwargs)\n",
        "        self.output_dim = output_dim\n",
        "        self.fuzzy_degree = nn.Parameter(torch.empty(output_dim))\n",
        "        self.sigma = nn.Parameter(torch.ones(output_dim))\n",
        "        self.reset_parameters()\n",
        "        \n",
        "    def reset_parameters(self) -> None:\n",
        "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
        "        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
        "        # https://github.com/pytorch/pytorch/issues/57109\n",
        "        nn.init.uniform_(self.fuzzy_degree)\n",
        "\n",
        "    def forward(self, input, **kwargs):\n",
        "        x = torch.repeat_interleave(torch.unsqueeze(input,-1), self.output_dim, dim=-1)\n",
        "        fuzzy_out = torch.exp(\n",
        "                      -torch.sum(\n",
        "                          torch.square((x-self.fuzzy_degree)/(self.sigma**2))            \n",
        "                          ,dim=-2, keepdims=False)\n",
        "              )\n",
        "        return fuzzy_out\n",
        "\n",
        "\n",
        "class FuzzyRuleLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim: int, output_dim,**kwargs):\n",
        "        super(FuzzyRuleLayer, self).__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.layers = nn.ModuleList([\n",
        "            FuzzyLayer(output_dim) for _ in range(input_dim)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch_size, input_dim = input.size()\n",
        "        an=torch.ones(batch_size, self.output_dim)\n",
        "        for layer in self.layers:\n",
        "            an=an*layer(input)\n",
        "        return an\n",
        "\n",
        "z=FuzzyRuleLayer(5,35)\n",
        "y=torch.ones(3,5)\n",
        "print(z(y).shape)\n",
        "print(z(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "227Lz-QqbOQK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icDOr3H9HlHY"
      },
      "source": [
        "## Try to add MyLSTM to fairseq (CPU only)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnFBO2ZoZoWu"
      },
      "source": [
        "### Some notes\n",
        "+ Follow the tutorial at https://fairseq.readthedocs.io/en/latest/tutorial_simple_lstm.html, add `@register_model` and `@register_model_architecture` to appropriate class.\n",
        "\n",
        "+ That's not all, you must either:\n",
        "\n",
        "1. Move your `model_name.py` to `fairseq/models`\n",
        "\n",
        "2. Inside your directory (supposed `user_dir`), create a new folder named `models` and move your file there. In the command line (such as `fairseq-train`), specify `--user-dir /path/to/user_dir`\n",
        "\n",
        "Because each time the command line tool runs, it must run some bootstrap code to get the current model list, parse your arguments, etc. Therefore you must follow some of their rules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85WWH0DhawMB",
        "outputId": "4da5f279-965c-41fd-bbc1-a1b8ac8e02a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'my_transformer' already exists and is not an empty directory.\n",
            "mkdir: cannot create directory ‘mymodel’: File exists\n",
            "/content/mymodel\n",
            "mkdir: cannot create directory ‘models’: File exists\n",
            "/content/mymodel/models\n"
          ]
        }
      ],
      "source": [
        "# Make mymodel path and prepare my_model (in this case `MyLSTM.py`) in user_dir (in this case `mymodel`)\n",
        "%cd /content\n",
        "!rm -rf my_transformer\n",
        "!git clone https://github.com/gigajet/transformer my_transformer\n",
        "!mkdir mymodel\n",
        "%cd mymodel\n",
        "!mkdir models\n",
        "%cd models \n",
        "!cp /content/my_transformer/MyFairseqLSTM.py .\n",
        "!cp -r /content/my_transformer/layer .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbnkII_hHqIj",
        "outputId": "b383bf6e-a5b0-413a-9b45-510b0367167c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "2022-04-22 10:02:58 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2022-04-22 10:02:59 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/content/mymodel', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 12000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='mylstm_default', adam_betas=(0.9, 0.999), adam_eps=1e-08, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='mylstm_default', azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='/content/drive/MyDrive/translation/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_dropout=0.2, decoder_embed_dim=256, decoder_hidden_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_dropout=0.2, encoder_embed_dim=256, encoder_hidden_dim=256, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.005], lr_scheduler='fixed', lr_shrink=0.5, max_epoch=1, max_source_positions=1024, max_target_positions=1024, max_tokens=12000, max_tokens_valid=12000, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/content/mymodel', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_updates=0, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/content/drive/MyDrive/translation/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.005]}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.5, 'warmup_updates': 0, 'lr': [0.005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-04-22 10:03:01 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types\n",
            "2022-04-22 10:03:01 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types\n",
            "MyLSTM(\n",
            "  (encoder): SimpleLSTMEncoder(\n",
            "    (embed_tokens): Embedding(8848, 256, padding_idx=1)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "  )\n",
            "  (decoder): SimpleLSTMDecoder(\n",
            "    (embed_tokens): Embedding(6632, 256, padding_idx=1)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (lstm): LSTM(512, 256)\n",
            "    (output_projection): Linear(in_features=256, out_features=6632, bias=True)\n",
            "  )\n",
            ")\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | MyLSTM(\n",
            "  (encoder): SimpleLSTMEncoder(\n",
            "    (embed_tokens): Embedding(8848, 256, padding_idx=1)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (lstm): LSTM(256, 256, batch_first=True)\n",
            "  )\n",
            "  (decoder): SimpleLSTMDecoder(\n",
            "    (embed_tokens): Embedding(6632, 256, padding_idx=1)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "    (lstm): LSTM(512, 256)\n",
            "    (output_projection): Linear(in_features=256, out_features=6632, bias=True)\n",
            "  )\n",
            ")\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | model: MyLSTM\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | num. shared model params: 6,982,120 (num. trained: 6,982,120)\n",
            "2022-04-22 10:03:01 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-04-22 10:03:01 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/valid.de-en.de\n",
            "2022-04-22 10:03:01 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/valid.de-en.en\n",
            "2022-04-22 10:03:01 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/translation/iwslt14.tokenized.de-en valid de-en 7283 examples\n",
            "2022-04-22 10:03:04 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-04-22 10:03:04 | INFO | fairseq.utils | rank   0: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               \n",
            "2022-04-22 10:03:04 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-04-22 10:03:04 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-04-22 10:03:04 | INFO | fairseq_cli.train | max tokens per device = 12000 and max sentences per device = None\n",
            "2022-04-22 10:03:04 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
            "2022-04-22 10:03:04 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
            "2022-04-22 10:03:04 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-04-22 10:03:04 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/train.de-en.de\n",
            "2022-04-22 10:03:04 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: /content/drive/MyDrive/translation/iwslt14.tokenized.de-en/train.de-en.en\n",
            "2022-04-22 10:03:04 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/translation/iwslt14.tokenized.de-en train de-en 160239 examples\n",
            "2022-04-22 10:03:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 388\n",
            "epoch 001:   0% 0/388 [00:00<?, ?it/s]2022-04-22 10:03:04 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-04-22 10:03:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/bin/fairseq-train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 316, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 856, in train_step\n",
            "    raise e\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 830, in train_step\n",
            "    **extra_kwargs,\n",
            "  File \"/content/fairseq/fairseq/tasks/fairseq_task.py\", line 512, in train_step\n",
            "    loss, sample_size, logging_output = criterion(model, sample)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/fairseq/fairseq/criterions/cross_entropy.py\", line 35, in forward\n",
            "    net_output = model(**sample[\"net_input\"])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/fairseq/fairseq/models/fairseq_model.py\", line 322, in forward\n",
            "    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/mymodel/models/MyFairseqLSTM.py\", line 60, in forward\n",
            "    x = nn.utils.rnn.pack_padded_sequence(x, src_lengths, batch_first=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/utils/rnn.py\", line 249, in pack_padded_sequence\n",
            "    _VF._pack_padded_sequence(input, lengths, batch_first)\n",
            "RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor\n"
          ]
        }
      ],
      "source": [
        "# Train the model, please change --max-epoch depends whether you use gpu/cpu\n",
        "%env CUDA_VISIBLE_DEVICES=0 \n",
        "!fairseq-train \\\n",
        "    /content/drive/MyDrive/translation/iwslt14.tokenized.de-en \\\n",
        "    --encoder-dropout 0.2 --decoder-dropout 0.2 \\\n",
        "    --optimizer adam --lr 0.005 --lr-shrink 0.5 \\\n",
        "    --max-epoch 1 \\\n",
        "    --max-tokens 12000 \\\n",
        "    --save-dir checkpoints \\\n",
        "    --user-dir /content/mymodel \\\n",
        "    --arch mylstm_default \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wp9VGKqQVPb"
      },
      "source": [
        "## Add MyTransformer to fairseq and train it using IWSLT15 en-vi dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfQorQ23Qf24",
        "outputId": "e32c2489-ffe9-4055-dab7-7ed881a79546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'my_transformer'...\n",
            "remote: Enumerating objects: 215, done.\u001b[K\n",
            "remote: Counting objects: 100% (215/215), done.\u001b[K\n",
            "remote: Compressing objects: 100% (147/147), done.\u001b[K\n",
            "remote: Total 215 (delta 118), reused 158 (delta 61), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (215/215), 27.84 MiB | 38.31 MiB/s, done.\n",
            "Resolving deltas: 100% (118/118), done.\n",
            "mkdir: cannot create directory ‘mymodel’: File exists\n",
            "/content/mymodel\n",
            "mkdir: cannot create directory ‘models’: File exists\n",
            "/content/mymodel/models\n",
            "/content/fairseq\n"
          ]
        }
      ],
      "source": [
        "# Make mymodel path and prepare my_model (in this case `MyLSTM.py`) in user_dir (in this case `mymodel`)\n",
        "%cd /content\n",
        "!rm -rf my_transformer\n",
        "!git clone https://github.com/gigajet/transformer my_transformer\n",
        "!mkdir mymodel\n",
        "%cd mymodel\n",
        "!mkdir models\n",
        "%cd models \n",
        "!cp /content/my_transformer/MyFairseqTransformer.py .\n",
        "!cp /content/my_transformer/nnFairseqTransformer.py .\n",
        "!cp -r /content/my_transformer/layer .\n",
        "%cd /content/fairseq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hv4UomOGyJqX"
      },
      "outputs": [],
      "source": [
        "# fix the import layer to mymodel.layer\n",
        "# F*** PYTHON3 IMPORT\n",
        "!find /content/mymodel -type f -exec sed -i \"s/from layer\\./from mymodel.models.layer./g\" {} \\;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w81ozS3NQf3F",
        "outputId": "f45b2122-c9bc-41ac-d129-b70609894522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n",
            "2022-05-06 15:12:28 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-05-06 15:12:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '/content/mymodel', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='nntransformer_default', adam_betas='(0.9, 0.98)', adam_eps=1e-08, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='nntransformer_default', azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='/content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', device_id=0, dim_feedforward=2048, dim_model=512, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_smoothing=0.1, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=100, max_source_positions=1024, max_src_len=16378, max_target_positions=1024, max_tgt_len=16378, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_head=8, num_layer=6, num_shards=1, num_workers=1, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir='/content/mymodel', valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': '/content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-05-06 15:12:30 | INFO | fairseq.tasks.translation | [en] dictionary: 7656 types\n",
            "2022-05-06 15:12:30 | INFO | fairseq.tasks.translation | [vi] dictionary: 6656 types\n",
            "2022-05-06 15:12:34 | INFO | fairseq_cli.train | NNTransformer(\n",
            "  (encoder): NNTransformerEncoder(\n",
            "    (embedding): PositionalEncodedEmbedding(\n",
            "      (input_embedding): Embedding(7656, 512, padding_idx=1)\n",
            "    )\n",
            "    (nn_encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (2): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (3): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (4): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (5): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): NNTransformerDecoder(\n",
            "    (embedding): PositionalEncodedEmbedding(\n",
            "      (input_embedding): Embedding(6656, 512, padding_idx=1)\n",
            "    )\n",
            "    (nn_decoder): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (1): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (2): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (3): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (4): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "        (5): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.3, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.3, inplace=False)\n",
            "          (dropout2): Dropout(p=0.3, inplace=False)\n",
            "          (dropout3): Dropout(p=0.3, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=6656, bias=True)\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            ")\n",
            "2022-05-06 15:12:34 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-05-06 15:12:34 | INFO | fairseq_cli.train | model: NNTransformer\n",
            "2022-05-06 15:12:34 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-05-06 15:12:34 | INFO | fairseq_cli.train | num. shared model params: 54,880,768 (num. trained: 54,880,768)\n",
            "2022-05-06 15:12:34 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-05-06 15:12:34 | INFO | fairseq.data.data_utils | loaded 5,089 examples from: /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset/valid.en-vi.en\n",
            "2022-05-06 15:12:34 | INFO | fairseq.data.data_utils | loaded 5,089 examples from: /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset/valid.en-vi.vi\n",
            "2022-05-06 15:12:34 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset valid en-vi 5089 examples\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.0.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.0.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.1.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.1.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.1.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.2.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.2.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.2.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.3.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.3.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.3.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.4.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.4.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.4.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.5.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.5.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- encoder.nn_encoder.layers.5.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.0.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.1.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.2.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.3.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.4.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.self_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.self_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.self_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.multihead_attn.q_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.multihead_attn.k_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | detected shared parameter: encoder.nn_encoder.layers.0.self_attn.q_proj_weight <- decoder.nn_decoder.layers.5.multihead_attn.v_proj_weight\n",
            "2022-05-06 15:12:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-05-06 15:12:34 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2022-05-06 15:12:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-05-06 15:12:34 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-05-06 15:12:34 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | Preparing to load checkpoint /content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer/checkpoint_last.pt\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | No existing checkpoint found /content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer/checkpoint_last.pt\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-05-06 15:12:34 | INFO | fairseq.data.data_utils | loaded 111,966 examples from: /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset/train.en-vi.en\n",
            "2022-05-06 15:12:34 | INFO | fairseq.data.data_utils | loaded 111,966 examples from: /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset/train.en-vi.vi\n",
            "2022-05-06 15:12:34 | INFO | fairseq.tasks.translation | /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset train en-vi 111966 examples\n",
            "2022-05-06 15:12:34 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2022-05-06 15:12:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 816\n",
            "epoch 001:   0% 0/816 [00:00<?, ?it/s]2022-05-06 15:12:34 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-05-06 15:12:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/content/fairseq/fairseq/utils.py:375: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/bin/fairseq-train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 316, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/content/fairseq/fairseq/trainer.py\", line 830, in train_step\n",
            "    **extra_kwargs,\n",
            "  File \"/content/fairseq/fairseq/tasks/fairseq_task.py\", line 516, in train_step\n",
            "    optimizer.backward(loss)\n",
            "  File \"/content/fairseq/fairseq/optim/fairseq_optimizer.py\", line 95, in backward\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 363, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# Use MyTransformer to train en-vi data\n",
        "# Train the model, please change --max-epoch depends whether you use gpu/cpu\n",
        "# see stop-time-hours\n",
        "%env CUDA_VISIBLE_DEVICES=0 \n",
        "!fairseq-train \\\n",
        "    /content/drive/MyDrive/translation/iwslt15.tokenized.en-vi/dataset \\\n",
        "    --arch nntransformer_default \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-epoch 100 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --eval-bleu \\\n",
        "    --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses \\\n",
        "    --eval-bleu-remove-bpe \\\n",
        "    --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir /content/drive/MyDrive/iwslt15.tokenized.en-vi/checkpoints-nntransformer \\\n",
        "    --user-dir /content/mymodel\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_OBkDDiNQPuD",
        "CBYOOYRFbojI",
        "uuVoveca1ATX",
        "YYLNU2e0Ow0d",
        "KsmOKipnchcY",
        "N0Wbr-bJW4-r",
        "icDOr3H9HlHY"
      ],
      "name": "running fairseq",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}